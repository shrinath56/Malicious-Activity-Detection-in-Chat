# -*- coding: utf-8 -*-
"""Threat Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vN155tGrI4BWcNoB2OHIsnt9RSQuRli4
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import pandas as pd 
import numpy as np
import re
import string
import matplotlib.pyplot as plt
import pickle
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.feature_selection import chi2

data1 = pd.read_csv("drive/MyDrive/Final_Project/merged_final1.csv", encoding='latin-1') 
data = data1.sample(frac=1) #to shuffle data
print(data)

# removing everything except alphabets`
data['Comments'] = data['Comments'].str.replace("[^a-zA-Z#]", " ")

#converting to string
data['Comments'] = data['Comments'].apply(str)

# removing short words
data['Comments'] = data['Comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))

# make all text lowercase
data['Comments'] = data['Comments'].apply(lambda x: x.lower())

data['Comments']

stop_words = stopwords.words('english')

# tokenization
tokens = data['Comments'].apply(lambda x: x.split())

# remove stop-words
filtered_words = tokens.apply(lambda x: [item for item in x if item not in stop_words])

print(filtered_words)
data['Comments'] = [' '.join(sen) for sen in filtered_words]
data['tokens'] = filtered_words
#data['Comments']
#data['tokens']
#filtered_words

data.groupby('Frequency').size()

X = data.Comments
y = data.Frequency
print(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)

print(X_train)

print("Train set has total {0} entries with {1:.2f}% nonflagged, {2:.2f}% flagged, {3:.2f}% Highly flagged".format(len(X_train),
                                                                             (len(X_train[y_train == 0]) / (len(X_train)))*100,
                                                                            (len(X_train[y_train == 50]) / (len(X_train)))*100,
                                                                            (len(X_train[y_train == 100]) / (len(X_train)))*100))

print("Test set has total {0} entries with {1:.2f}% nonflagged, {2:.2f}% flagged, {3:.2f}% Highly flagged".format(len(X_test),
                                                                             (len(X_test[y_test == 0]) / (len(X_test)))*100,
                                                                            (len(X_test[y_test == 50]) / (len(X_test)))*100,
                                                                            (len(X_test[y_test == 100]) / (len(X_test)))*100))

def accuracy_summary(pipeline, X_train, y_train, X_test, y_test):
    sentiment_fit = pipeline.fit(X_train, y_train)
    y_pred = sentiment_fit.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print("accuracy score: {0:.2f}%".format(accuracy*100))
    return accuracy

rf = RandomForestClassifier()
n_features = np.arange(7104,35523,7104)

def nfeature_accuracy_checker_rf(vectorizer, n_features=n_features, ngram_range=(1, 3), classifier=rf):
    result = []
    print(classifier)
    print("\n")
    for n in n_features:
        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)
        checker_pipeline = Pipeline([
            ('vectorizer', vectorizer),
            ('classifier', classifier)
        ])
        print("Test result for {} features".format(n))
        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)
        result.append((n,nfeature_accuracy))
    return result

clf = DecisionTreeClassifier()

def nfeature_accuracy_checker_clf(vectorizer, n_features=n_features, ngram_range=(1, 3), classifier=clf):
    result = []
    print(classifier)
    print("\n")
    for n in n_features:
        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)
        checker_pipeline = Pipeline([
            ('vectorizer', vectorizer),
            ('classifier', classifier)
        ])
        print("Test result for {} features".format(n))
        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)
        result.append((n,nfeature_accuracy))
    return result

knn = KNeighborsClassifier(n_neighbors=5)

def nfeature_accuracy_checker_knn(vectorizer, n_features=n_features, ngram_range=(1, 3), classifier=knn):
    result = []
    print(classifier)
    print("\n")
    for n in n_features:
        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)
        checker_pipeline = Pipeline([
            ('vectorizer', vectorizer),
            ('classifier', classifier)
        ])
        print("Test result for {} features".format(n))
        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)
        result.append((n,nfeature_accuracy))
    return result

tfidf = TfidfVectorizer()
print("Result for trigram with stop words (Tfidf)\n")
feature_result_tgt_rf = nfeature_accuracy_checker_rf(vectorizer=tfidf,ngram_range=(1, 3))
feature_result_tgt_clf = nfeature_accuracy_checker_clf(vectorizer=tfidf,ngram_range=(1, 3))
feature_result_tgt_knn = nfeature_accuracy_checker_knn(vectorizer=tfidf,ngram_range=(1, 3))

tfidf = TfidfVectorizer()
feature_result_tgt_clf = nfeature_accuracy_checker_clf(vectorizer=tfidf,ngram_range=(1, 3))

tfidf = TfidfVectorizer()
feature_result_tgt_knn = nfeature_accuracy_checker_knn(vectorizer=tfidf,ngram_range=(1, 3))

print(feature_result_tgt_rf)
print(feature_result_tgt_clf)
print(feature_result_tgt_knn)

pipeline = Pipeline([
        ('vectorizer', tfidf),
        ('classifier', rf)
    ])
sentiment_fit = pipeline.fit(X_train, y_train)
y_pred = sentiment_fit.predict(X_test)
#print(X_test,y_test)
#print(y_pred)

print(classification_report(y_test, y_pred, target_names=['nonflagged','flagged','highly flagged']))

from sklearn.metrics import plot_confusion_matrix

# Plot non-normalized confusion matrix
titles_options = [("Confusion matrix, without normalization", 'd', None),
                  ("Normalized confusion matrix", '.2f', 'true')]
for title, values_format, normalize in titles_options:
    disp = plot_confusion_matrix(sentiment_fit, X_test, y_test,
                                 cmap=plt.cm.Blues,
                                 values_format=values_format,
                                 normalize=normalize)
    disp.ax_.set_title(title)

    #print(title)
    #print(disp.confusion_matrix)

plt.show()

tfidf = TfidfVectorizer(max_features=20000,ngram_range=(1, 3))
X_tfidf = tfidf.fit_transform(data.Comments)
y = data.Frequency
chi2score = chi2(X_tfidf, y)[0]
print(chi2score)

# Commented out IPython magic to ensure Python compatibility.

# %matplotlib inline

plt.figure(figsize=(12,8))
scores = list(zip(tfidf.get_feature_names(), chi2score))
chi2 = sorted(scores, key=lambda x:x[1])
topchi2 = list(zip(*chi2[-25:-5]))
x = range(len(topchi2[1]))
labels = topchi2[0]
plt.barh(x,topchi2[1], align='center', alpha=0.5)
plt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)
plt.yticks(x, labels)
plt.xlabel('$\chi^2$')
plt.show();

with open('sent.pkl', 'wb') as pickle_out:
     pickle.dump(sentiment_fit, pickle_out)

#with open('sent.pkl', 'rb') as pickle_in:
   #  unpickled_sent = pickle.load(pickle_in)

classes = ['Non-Flagged', 'Flagged', 'Highly-Flagged']
y_new = []
y_new1 = []
y_new2 = []
#text = ['You have too much ego of your face I will sue you'] #right
#text = ['Fuck the shit, you bastard'] #right
#text = ['hello'] #right
#text = ['you bastard']  #right
#text = ['Fuck you bastard']   #right
#text = ['I have your nude pics']   #right
#text = ["I am begging leave me alone, don't upload my nudes"] #wrong

#text = ['hello, do you remember me', 'yes ofcourse, how can I forgot you', 'you are the one who had destroyed my life', 'You have too much ego of your face I will sue you', 'Fuck the shit, you bastard', 'hello', 'you bastard', 'I have your nude pics', 'I am begging leave me alone, don\'t upload my nudes']
#text = ['hello, do you remember me', 'yes ofcourse, how can I forgot you', 'you are the one who had destroyed my life', 'srry about that but still i love you', 'ohh silly, just shut you mouth', 'really I love you ', 'just fuck off', 'leaving you was the biggest mistake of my life', 'yes, you had made that mistake', 'srry about that', 'now you have to face it']
text = ['hello', 'how are you', 'i am fine and you?', 'I am also fine', 'what are you doing nowadays', 'i am working in a mnc as a software engineer', 'what about you, you were thinking of some startup', 'yeah, I am workinng on it and it will be ready by next month', 'ohh good to see that, all the best for it', 'thanks mate, let\'s hope for the best']
#y_prob = sentiment_fit.predict(text)
#print(y_prob)

for i in text:
  y_prob =sentiment_fit.predict([i])
  y_prob1 =sentiment_fit1.predict([i])
  y_prob2 =sentiment_fit2.predict([i])
  y_new.append(y_prob)
  y_new1.append(y_prob1)
  y_new2.append(y_prob2)

print(y_new)
avg = sum(y_new)/len(y_new)
print(avg)
if (avg > 50):
  print("Malicious activity detected")
else:
  print("No Malicious activity detected")

print(y_new1)
avg1 = sum(y_new1)/len(y_new1)
print(avg1)
if (avg1 > 50):
  print("Malicious activity detected")
else:
  print("No Malicious activity detected")

print(y_new2)
avg2 = sum(y_new2)/len(y_new2)
print(avg2)
if (avg2 > 50):
  print("Malicious activity detected")
else:
  print("No Malicious activity detected")

